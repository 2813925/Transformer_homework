# ğŸ› Seq2Seqæ¨¡å¼Bugä¿®å¤è¯´æ˜

## é—®é¢˜æè¿°

è¿è¡Œ `bash scripts/run.sh seq2seq` æ—¶å‡ºç°é”™è¯¯ï¼š
```
RuntimeError: The size of tensor a (128) must match the size of tensor b (8192) at non-singleton dimension 1
```

## é—®é¢˜åŸå› 

åœ¨åŸå§‹å®ç°ä¸­ï¼Œmaskçš„ç»´åº¦å¤„ç†æœ‰é—®é¢˜ï¼š

1. **MultiHeadAttentionæœŸæœ›çš„maskæ ¼å¼**ï¼š
   - è¾“å…¥: `[batch_size, seq_len_q, seq_len_k]`
   - å†…éƒ¨ä¼šunsqueeze(1)å˜æˆ: `[batch_size, 1, seq_len_q, seq_len_k]`

2. **åŸå§‹ä»£ç çš„é—®é¢˜**ï¼š
   - `make_src_mask()`è¿”å›: `[batch_size, 1, 1, src_len]`
   - `make_tgt_mask()`è¿”å›: `[batch_size, 1, tgt_len, tgt_len]`
   - è¿™äº›maskåœ¨MultiHeadAttentionä¸­åˆè¢«unsqueeze(1)ï¼Œå¯¼è‡´ç»´åº¦ä¸åŒ¹é…

3. **Cross-attentionçš„ç‰¹æ®Šæ€§**ï¼š
   - Queryæ¥è‡ªdecoder (é•¿åº¦tgt_len)
   - Key/Valueæ¥è‡ªencoder (é•¿åº¦src_len)
   - éœ€è¦ä¸“é—¨çš„cross_mask: `[batch_size, tgt_len, src_len]`

## è§£å†³æ–¹æ¡ˆ

### ä¿®æ”¹1: make_src_mask()

**ä¿®æ”¹å‰**:
```python
def make_src_mask(self, src):
    src_mask = (src != 0).unsqueeze(1).unsqueeze(2)
    # è¿”å› [batch_size, 1, 1, src_len]
    return src_mask
```

**ä¿®æ”¹å**:
```python
def make_src_mask(self, src):
    """åˆ›å»ºæºåºåˆ—padding mask
    Returns:
        mask: [batch_size, 1, src_len] - é€‚é…MultiHeadAttentionçš„è¾“å…¥æ ¼å¼
    """
    src_mask = (src != 0).unsqueeze(1)  # [batch_size, 1, src_len]
    return src_mask
```

### ä¿®æ”¹2: make_tgt_mask()

**ä¿®æ”¹å‰**:
```python
def make_tgt_mask(self, tgt):
    batch_size, tgt_len = tgt.shape
    tgt_pad_mask = (tgt != 0).unsqueeze(1).unsqueeze(2)  # [batch_size, 1, 1, tgt_len]
    tgt_sub_mask = torch.tril(torch.ones((tgt_len, tgt_len), device=tgt.device)).bool()
    tgt_mask = tgt_pad_mask & tgt_sub_mask
    return tgt_mask
```

**ä¿®æ”¹å**:
```python
def make_tgt_mask(self, tgt):
    """åˆ›å»ºç›®æ ‡åºåˆ—çš„look-ahead mask + padding mask
    Returns:
        mask: [batch_size, tgt_len, tgt_len]
    """
    batch_size, tgt_len = tgt.shape
    
    # Padding mask: [batch_size, 1, tgt_len]
    tgt_pad_mask = (tgt != 0).unsqueeze(1)
    
    # Look-ahead mask: [tgt_len, tgt_len]
    tgt_sub_mask = torch.tril(
        torch.ones((tgt_len, tgt_len), device=tgt.device)
    ).bool()
    
    # ç»„åˆ: [batch_size, tgt_len, tgt_len]
    tgt_mask = tgt_pad_mask.unsqueeze(1) & tgt_sub_mask.unsqueeze(0)
    
    return tgt_mask
```

### ä¿®æ”¹3: æ–°å¢make_cross_mask()

```python
def make_cross_mask(self, src, tgt):
    """åˆ›å»ºcross-attentionçš„mask
    Args:
        src: [batch_size, src_len]
        tgt: [batch_size, tgt_len]
    Returns:
        mask: [batch_size, tgt_len, src_len]
    """
    src_mask = (src != 0).unsqueeze(1)  # [batch_size, 1, src_len]
    tgt_len = tgt.size(1)
    cross_mask = src_mask.expand(-1, tgt_len, -1)
    return cross_mask
```

### ä¿®æ”¹4: forward()å‡½æ•°

**ä¿®æ”¹å‰**:
```python
def forward(self, src, tgt):
    src_mask = self.make_src_mask(src)
    tgt_mask = self.make_tgt_mask(tgt)
    
    enc_output = self.encode(src, src_mask)
    dec_output = self.decode(tgt, enc_output, src_mask, tgt_mask)  # âŒ é”™è¯¯
    
    output = self.fc_out(dec_output)
    return output
```

**ä¿®æ”¹å**:
```python
def forward(self, src, tgt):
    src_mask = self.make_src_mask(src)  # [batch_size, 1, src_len]
    tgt_mask = self.make_tgt_mask(tgt)  # [batch_size, tgt_len, tgt_len]
    cross_mask = self.make_cross_mask(src, tgt)  # [batch_size, tgt_len, src_len]
    
    enc_output = self.encode(src, src_mask)
    dec_output = self.decode(tgt, enc_output, cross_mask, tgt_mask)  # âœ… æ­£ç¡®
    
    output = self.fc_out(dec_output)
    return output
```

### ä¿®æ”¹5: decode()å’ŒDecoderLayer

```python
# decodeå‡½æ•°
def decode(self, tgt, enc_output, cross_mask, tgt_mask):  # å‚æ•°åæ”¹ä¸ºcross_mask
    x = self.tgt_embedding(tgt) * math.sqrt(self.d_model)
    x = self.pos_encoding(x)
    
    for layer in self.decoder_layers:
        x = layer(x, enc_output, cross_mask, tgt_mask)  # ä½¿ç”¨cross_mask
    
    return x

# DecoderLayer.forward
def forward(self, x, enc_output, cross_mask=None, tgt_mask=None):
    # Self-attention
    self_attn_output, _ = self.self_attn(x, x, x, tgt_mask)
    x = self.norm1(x + self_attn_output)
    
    # Cross-attention (ä½¿ç”¨cross_mask)
    cross_attn_output, _ = self.cross_attn(x, enc_output, enc_output, cross_mask)
    x = self.norm2(x + cross_attn_output)
    
    # FFN
    ffn_output = self.ffn(x)
    x = self.norm3(x + ffn_output)
    
    return x
```

## å¦‚ä½•åº”ç”¨ä¿®å¤

### æ–¹æ³•1: æ‰‹åŠ¨ä¿®æ”¹ä»£ç 

1. æ‰“å¼€ `src/model.py`
2. æ‰¾åˆ° `Transformer` ç±»
3. æŒ‰ç…§ä¸Šè¿°è¯´æ˜ä¿®æ”¹5ä¸ªéƒ¨åˆ†

### æ–¹æ³•2: ä¸‹è½½ä¿®å¤åçš„æ–‡ä»¶

ä¿®å¤åçš„å®Œæ•´æ–‡ä»¶å·²ä¿å­˜åœ¨outputsç›®å½•ï¼š
- è·¯å¾„: `transformer_project/src/model.py`
- ç›´æ¥å¤åˆ¶åˆ°ä½ çš„é¡¹ç›®ä¸­æ›¿æ¢åŸæ–‡ä»¶

### æ–¹æ³•3: ä½¿ç”¨å¤‡ä»½æ¢å¤ï¼ˆå¦‚æœéœ€è¦ï¼‰

å¦‚æœä¿®æ”¹å‡ºé”™ï¼Œå¯ä»¥ä»å¤‡ä»½æ¢å¤ï¼š
```bash
cp src/model.py.backup src/model.py
```

## éªŒè¯ä¿®å¤

ä¿®å¤åï¼Œé‡æ–°è¿è¡Œï¼š
```bash
bash scripts/run.sh seq2seq
```

åº”è¯¥çœ‹åˆ°ï¼š
```
ä½¿ç”¨è®¾å¤‡: cuda
å‡†å¤‡æ•°æ®...
åˆ›å»ºæ¨¡å‹...
æ¨¡å‹æ€»å‚æ•°: 29,603,052
å¯è®­ç»ƒå‚æ•°: 29,603,052
å¼€å§‹è®­ç»ƒ...
Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 275/275 [00:XX<00:00, X.XXit/s, loss=6.XXXX, ppl=XXX.XX]
Epoch: 01 | Time: XXXs
Train Loss: 6.XXXX | Train PPL: XXX.XX
Valid Loss: 6.XXXX | Valid PPL: XXX.XX
```

## Maskç»´åº¦æ€»ç»“

| Maskç±»å‹ | ç»´åº¦ | ç”¨é€” |
|---------|------|------|
| src_mask | [batch, 1, src_len] | Encoder self-attention |
| tgt_mask | [batch, tgt_len, tgt_len] | Decoder self-attention (masked) |
| cross_mask | [batch, tgt_len, src_len] | Decoder cross-attention |

**å…³é”®ç‚¹**ï¼š
- MultiHeadAttentionæœŸæœ›è¾“å…¥maskä¸º3D: `[batch, seq_q, seq_k]`
- å†…éƒ¨ä¼šè‡ªåŠ¨unsqueeze(1)å˜æˆ4D: `[batch, 1, seq_q, seq_k]`
- ä¸è¦åœ¨make_maskå‡½æ•°ä¸­é¢„å…ˆåšè¿™ä¸ªunsqueezeï¼

## å…¶ä»–æ³¨æ„äº‹é¡¹

1. **Encoder-onlyæ¨¡å¼ä¸å—å½±å“**ï¼š
   - `TransformerEncoderOnly`ç±»ä½¿ç”¨ç®€å•çš„self-attention
   - maskå¤„ç†æ›´ç®€å•ï¼Œä¸éœ€è¦cross_mask

2. **å¦‚æœé‡åˆ°å…¶ä»–ç»´åº¦é”™è¯¯**ï¼š
   - æ£€æŸ¥batch_sizeæ˜¯å¦æ­£ç¡®
   - æ£€æŸ¥åºåˆ—é•¿åº¦æ˜¯å¦åœ¨max_lenèŒƒå›´å†…
   - ä½¿ç”¨`print(tensor.shape)`è°ƒè¯•

3. **æ€§èƒ½ä¼˜åŒ–**ï¼š
   - ä¿®å¤åçš„ä»£ç ä¸ä¼šå½±å“æ€§èƒ½
   - è®­ç»ƒé€Ÿåº¦åº”è¯¥ä¸ä¹‹å‰ç›¸åŒ

## æµ‹è¯•å»ºè®®

ä¿®å¤åï¼Œå»ºè®®è¿›è¡Œå¿«é€Ÿæµ‹è¯•ï¼š
```bash
# å¿«é€Ÿæµ‹è¯•ï¼ˆ3 epochsï¼‰
python src/train.py \
  --mode seq2seq \
  --epochs 3 \
  --batch_size 32 \
  --exp_name test_seq2seq

# å¦‚æœæˆåŠŸï¼Œå†è¿è¡Œå®Œæ•´è®­ç»ƒ
bash scripts/run.sh seq2seq
```

---

**ä¿®å¤å®Œæˆï¼** ç°åœ¨seq2seqæ¨¡å¼åº”è¯¥å¯ä»¥æ­£å¸¸è¿è¡Œäº†ã€‚

å¦‚æœ‰å…¶ä»–é—®é¢˜ï¼Œè¯·æ£€æŸ¥ï¼š
1. PyTorchç‰ˆæœ¬ (å»ºè®® >= 2.0.0)
2. CUDAæ˜¯å¦å¯ç”¨
3. æ•°æ®é›†æ˜¯å¦æ­£ç¡®åŠ è½½
