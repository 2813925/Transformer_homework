# ğŸ”§ Seq2Seq Bug ç»ˆæä¿®å¤æ–¹æ¡ˆ

## é—®é¢˜
```
RuntimeError: The size of tensor a (127) must match the size of tensor b (8128)
```

## æ ¹æœ¬åŸå› 
Maskç»´åº¦ä¸æ­£ç¡®ã€‚MultiHeadAttentionæœŸæœ›ï¼š
- è¾“å…¥mask: `[batch_size, seq_len_q, seq_len_k]`
- å†…éƒ¨ä¼šunsqueeze(1)å˜æˆ: `[batch_size, 1, seq_len_q, seq_len_k]`

## è§£å†³æ–¹æ¡ˆ

### ä¿®æ”¹1: make_src_mask()
æ‰¾åˆ°`make_src_mask`å‡½æ•°ï¼ˆå¤§çº¦åœ¨ç¬¬293è¡Œï¼‰ï¼Œå®Œæ•´æ›¿æ¢ä¸ºï¼š

```python
def make_src_mask(self, src):
    """åˆ›å»ºæºåºåˆ—padding mask
    Returns:
        mask: [batch_size, src_len, src_len]
    """
    # src: [batch_size, src_len]
    src_mask = (src != 0).unsqueeze(1)  # [batch_size, 1, src_len]
    src_mask = src_mask.expand(-1, src.size(1), -1)  # [batch_size, src_len, src_len]
    return src_mask
```

### ä¿®æ”¹2: make_tgt_mask()
æ‰¾åˆ°`make_tgt_mask`å‡½æ•°ï¼ˆå¤§çº¦åœ¨ç¬¬303è¡Œï¼‰ï¼Œå®Œæ•´æ›¿æ¢ä¸ºï¼š

```python
def make_tgt_mask(self, tgt):
    """åˆ›å»ºç›®æ ‡åºåˆ—çš„look-ahead mask + padding mask
    Returns:
        mask: [batch_size, tgt_len, tgt_len]
    """
    batch_size, tgt_len = tgt.shape
    
    # Padding mask: [batch_size, 1, tgt_len]
    tgt_pad_mask = (tgt != 0).unsqueeze(1)
    
    # Look-ahead mask: [tgt_len, tgt_len]
    tgt_sub_mask = torch.tril(
        torch.ones((tgt_len, tgt_len), device=tgt.device)
    ).bool()
    
    # ç»„åˆ: PyTorchä¼šè‡ªåŠ¨å¹¿æ’­
    tgt_mask = tgt_pad_mask & tgt_sub_mask
    # ç»“æœ: [batch_size, tgt_len, tgt_len]
    
    return tgt_mask
```

### ä¿®æ”¹3: make_cross_mask()
åœ¨`make_tgt_mask`å‡½æ•°ä¹‹åæ·»åŠ æ–°å‡½æ•°ï¼š

```python
def make_cross_mask(self, src, tgt):
    """åˆ›å»ºcross-attentionçš„mask
    Args:
        src: [batch_size, src_len]
        tgt: [batch_size, tgt_len]
    Returns:
        mask: [batch_size, tgt_len, src_len]
    """
    src_mask = (src != 0).unsqueeze(1)  # [batch_size, 1, src_len]
    tgt_len = tgt.size(1)
    cross_mask = src_mask.expand(-1, tgt_len, -1)  # [batch_size, tgt_len, src_len]
    return cross_mask
```

### ä¿®æ”¹4: forward()å‡½æ•°
æ‰¾åˆ°`forward`å‡½æ•°ï¼ˆå¤§çº¦åœ¨ç¬¬365è¡Œï¼‰ï¼Œä¿®æ”¹ä¸ºï¼š

```python
def forward(self, src, tgt):
    """
    Args:
        src: [batch_size, src_len]
        tgt: [batch_size, tgt_len]
    Returns:
        output: [batch_size, tgt_len, tgt_vocab_size]
    """
    # åˆ›å»ºmask
    src_mask = self.make_src_mask(src)  # [batch_size, src_len, src_len]
    tgt_mask = self.make_tgt_mask(tgt)  # [batch_size, tgt_len, tgt_len]
    cross_mask = self.make_cross_mask(src, tgt)  # [batch_size, tgt_len, src_len]
    
    # Encoder
    enc_output = self.encode(src, src_mask)
    
    # Decoder (æ³¨æ„ï¼šä½¿ç”¨cross_maskè€Œä¸æ˜¯src_mask)
    dec_output = self.decode(tgt, enc_output, cross_mask, tgt_mask)
    
    # è¾“å‡ºå±‚
    output = self.fc_out(dec_output)
    
    return output
```

### ä¿®æ”¹5: decode()å‡½æ•°
æ‰¾åˆ°`decode`å‡½æ•°ï¼ˆå¤§çº¦åœ¨ç¬¬353è¡Œï¼‰ï¼Œä¿®æ”¹å‚æ•°åï¼š

```python
def decode(self, tgt, enc_output, cross_mask, tgt_mask):
    """Decoderå‰å‘ä¼ æ’­"""
    # è¯åµŒå…¥ + ä½ç½®ç¼–ç 
    x = self.tgt_embedding(tgt) * math.sqrt(self.d_model)
    x = self.pos_encoding(x)
    
    # é€šè¿‡æ‰€æœ‰Decoderå±‚
    for layer in self.decoder_layers:
        x = layer(x, enc_output, cross_mask, tgt_mask)
    
    return x
```

### ä¿®æ”¹6: DecoderLayer.forward()
æ‰¾åˆ°DecoderLayerçš„forwardå‡½æ•°ï¼ˆå¤§çº¦åœ¨ç¬¬218è¡Œï¼‰ï¼Œä¿®æ”¹å‚æ•°åï¼š

```python
def forward(self, x, enc_output, cross_mask=None, tgt_mask=None):
    """
    Args:
        x: [batch_size, tgt_len, d_model]
        enc_output: [batch_size, src_len, d_model]
        cross_mask: [batch_size, tgt_len, src_len]
        tgt_mask: [batch_size, tgt_len, tgt_len]
    """
    # Maskedå¤šå¤´è‡ªæ³¨æ„åŠ› + æ®‹å·® + LayerNorm
    self_attn_output, _ = self.self_attn(x, x, x, tgt_mask)
    x = self.norm1(x + self_attn_output)
    
    # ç¼–ç å™¨-è§£ç å™¨æ³¨æ„åŠ› + æ®‹å·® + LayerNorm
    cross_attn_output, _ = self.cross_attn(x, enc_output, enc_output, cross_mask)
    x = self.norm2(x + cross_attn_output)
    
    # å‰é¦ˆç½‘ç»œ + æ®‹å·® + LayerNorm
    ffn_output = self.ffn(x)
    x = self.norm3(x + ffn_output)
    
    return x
```

## å¿«é€ŸéªŒè¯

ä¿®æ”¹åï¼Œåœ¨é¡¹ç›®æ ¹ç›®å½•è¿è¡Œï¼š

```bash
python -c "
import torch
import sys
sys.path.append('src')
from model import Transformer

model = Transformer(1000, 1000, d_model=256, n_heads=4)
src = torch.randint(1, 1000, (4, 10))
tgt = torch.randint(1, 1000, (4, 8))

src_mask = model.make_src_mask(src)
tgt_mask = model.make_tgt_mask(tgt)
cross_mask = model.make_cross_mask(src, tgt)

print('src_mask:', src_mask.shape, 'åº”è¯¥æ˜¯ [4, 10, 10]')
print('tgt_mask:', tgt_mask.shape, 'åº”è¯¥æ˜¯ [4, 8, 8]')
print('cross_mask:', cross_mask.shape, 'åº”è¯¥æ˜¯ [4, 8, 10]')

output = model(src, tgt)
print('output:', output.shape, 'åº”è¯¥æ˜¯ [4, 8, 1000]')
print('âœ“ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼')
"
```

## ä¸‹è½½ä¿®å¤åçš„æ–‡ä»¶

å®Œæ•´çš„ä¿®å¤åçš„`model.py`åœ¨outputsç›®å½•ä¸­ï¼Œå¯ä»¥ç›´æ¥ä¸‹è½½æ›¿æ¢ã€‚

## é‡æ–°è¿è¡Œ

```bash
bash scripts/run.sh seq2seq
```

åº”è¯¥çœ‹åˆ°æ­£å¸¸çš„è®­ç»ƒè¾“å‡ºï¼

---

**å…³é”®ç‚¹æ€»ç»“ï¼š**

| Maskç±»å‹ | æ­£ç¡®ç»´åº¦ | ç”¨é€” |
|---------|---------|------|
| src_mask | [B, src_len, src_len] | Encoderè‡ªæ³¨æ„åŠ› |
| tgt_mask | [B, tgt_len, tgt_len] | Decoderè‡ªæ³¨æ„åŠ›(masked) |
| cross_mask | [B, tgt_len, src_len] | Decoderäº¤å‰æ³¨æ„åŠ› |

**æ ¸å¿ƒé—®é¢˜ï¼š** ä¹‹å‰çš„ä»£ç ä¸­tgt_pad_maskåšäº†å¤šä½™çš„unsqueezeæ“ä½œï¼Œå¯¼è‡´ç»´åº¦é”™è¯¯ã€‚
