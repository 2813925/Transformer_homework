# Transformer ä»é›¶å®ç°

åŸºäºPyTorchçš„Transformerå®Œæ•´å®ç°ï¼ŒåŒ…å«Encoder-Decoderæ¶æ„ã€è®­ç»ƒæµç¨‹å’Œæ¶ˆèå®éªŒã€‚

## ğŸ“‹ é¡¹ç›®æ¦‚è¿°

æœ¬é¡¹ç›®ä»é›¶å®ç°äº†å®Œæ•´çš„Transformeræ¨¡å‹ï¼Œç”¨äº"å¤§æ¨¡å‹åŸºç¡€ä¸åº”ç”¨"è¯¾ç¨‹æœŸä¸­ä½œä¸šã€‚ä¸»è¦ç‰¹æ€§ï¼š

- âœ… **å®Œæ•´æ¶æ„**: å®ç°Encoderå’ŒDecoderï¼Œæ”¯æŒEncoder-onlyå’Œå®Œæ•´Seq2Seqæ¨¡å¼
- âœ… **æ ¸å¿ƒç»„ä»¶**: Multi-Head Attentionã€Position-wise FFNã€Residual+LayerNormã€Positional Encoding
- âœ… **è®­ç»ƒç¨³å®šæ€§**: AdamWä¼˜åŒ–å™¨ã€å­¦ä¹ ç‡warmupã€æ¢¯åº¦è£å‰ªã€æ ‡ç­¾å¹³æ»‘
- âœ… **æ¶ˆèå®éªŒ**: ç³»ç»Ÿæ€§æµ‹è¯•å„ç»„ä»¶å¯¹æ€§èƒ½çš„å½±å“
- âœ… **å®Œæ•´æ–‡æ¡£**: è¯¦ç»†çš„æ•°å­¦æ¨å¯¼å’Œå®ç°è¯´æ˜

## ğŸ—‚ï¸ é¡¹ç›®ç»“æ„

```
transformer_project/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ model.py           # Transformeræ¨¡å‹å®ç°
â”‚   â”œâ”€â”€ data_loader.py     # æ•°æ®åŠ è½½å’Œé¢„å¤„ç†
â”‚   â”œâ”€â”€ train.py           # è®­ç»ƒè„šæœ¬
â”‚   â””â”€â”€ ablation.py        # æ¶ˆèå®éªŒè„šæœ¬
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ run.sh             # è®­ç»ƒè¿è¡Œè„šæœ¬
â”‚   â””â”€â”€ download_data.py   # æ•°æ®ä¸‹è½½è„šæœ¬
â”œâ”€â”€ configs/               # é…ç½®æ–‡ä»¶ç›®å½•
â”œâ”€â”€ results/               # è®­ç»ƒç»“æœã€æ›²çº¿å›¾
â”œâ”€â”€ checkpoints/           # æ¨¡å‹æ£€æŸ¥ç‚¹
â”œâ”€â”€ data/                  # æ•°æ®é›†ç›®å½•
â”œâ”€â”€ requirements.txt       # ä¾èµ–åŒ…
â””â”€â”€ README.md             # æœ¬æ–‡ä»¶
```

## ğŸ“¦ æ•°æ®é›†

**ä½¿ç”¨æ•°æ®é›†**: WikiText-2

- **æè¿°**: çº¦2M tokensçš„è¯­è¨€å»ºæ¨¡æ•°æ®é›†ï¼Œæ¥è‡ªWikipediaä¼˜è´¨æ–‡ç« 
- **ä»»åŠ¡ç±»å‹**: è¯­è¨€å»ºæ¨¡ (Language Modeling)
- **æ•°æ®è§„æ¨¡**: 
  - è®­ç»ƒé›†: ~600K tokens
  - éªŒè¯é›†: ~60K tokens  
  - æµ‹è¯•é›†: ~60K tokens
- **ä¸‹è½½é“¾æ¥**: https://blog.salesforceairesearch.com/the-wikitext-long-term-dependency-language-modeling-dataset/

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒé…ç½®

```bash
# åˆ›å»ºcondaç¯å¢ƒ
conda create -n transformer python=3.10
conda activate transformer

# å®‰è£…ä¾èµ–
pip install torch numpy matplotlib tqdm pandas
# æˆ–
pip install -r requirements.txt
```

### 2. ä¸‹è½½æ•°æ®

**æ–¹æ³•1: ä½¿ç”¨è„šæœ¬è‡ªåŠ¨ä¸‹è½½**
```bash
python scripts/download_data.py
```

**æ–¹æ³•2: æ‰‹åŠ¨ä¸‹è½½**
1. è®¿é—® https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-raw-v1.zip
2. ä¸‹è½½å¹¶è§£å‹åˆ° `data/` ç›®å½•
3. ç¡®ä¿ç›®å½•ç»“æ„ä¸º:
```
data/
â”œâ”€â”€ wiki.train.tokens
â”œâ”€â”€ wiki.valid.tokens
â””â”€â”€ wiki.test.tokens
```

### 3. è®­ç»ƒæ¨¡å‹

**è®­ç»ƒEncoder-onlyæ¨¡å‹ (è¯­è¨€å»ºæ¨¡)**
```bash
bash scripts/run.sh encoder
```

**è®­ç»ƒå®Œæ•´Encoder-Decoderæ¨¡å‹**
```bash
bash scripts/run.sh seq2seq
```

**è¿è¡Œæ¶ˆèå®éªŒ**
```bash
bash scripts/run.sh ablation
```

### 4. è‡ªå®šä¹‰è®­ç»ƒ

```bash
python src/train.py \
  --data_dir data \
  --mode encoder \
  --d_model 256 \
  --n_heads 4 \
  --n_layers 4 \
  --d_ff 1024 \
  --batch_size 64 \
  --epochs 20 \
  --lr 0.0001 \
  --seed 42 \
  --exp_name my_experiment
```

## ğŸ“Š æ¨¡å‹æ¶æ„

### æ ¸å¿ƒç»„ä»¶

#### 1. Scaled Dot-Product Attention
```
Attention(Q, K, V) = softmax(QK^T / âˆšd_k) V
```

#### 2. Multi-Head Attention
```
MultiHead(Q, K, V) = Concat(head_1, ..., head_h)W^O
where head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)
```

#### 3. Position-wise Feed-Forward Network
```
FFN(x) = max(0, xW_1 + b_1)W_2 + b_2
```

#### 4. Positional Encoding
```
PE(pos, 2i) = sin(pos / 10000^(2i/d_model))
PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))
```

### æ¨¡å‹å‚æ•°

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ |
|------|--------|------|
| d_model | 256 | æ¨¡å‹ç»´åº¦ |
| n_heads | 4 | æ³¨æ„åŠ›å¤´æ•° |
| n_layers | 4 | Encoder/Decoderå±‚æ•° |
| d_ff | 1024 | å‰é¦ˆç½‘ç»œç»´åº¦ |
| dropout | 0.1 | Dropoutæ¦‚ç‡ |
| max_len | 128 | æœ€å¤§åºåˆ—é•¿åº¦ |

**å‚æ•°é‡ç»Ÿè®¡** (Encoder-only, é»˜è®¤é…ç½®):
- æ€»å‚æ•°: ~8.5M
- å¯è®­ç»ƒå‚æ•°: ~8.5M

## ğŸ”¬ å®éªŒè®¾ç½®

### è®­ç»ƒè¶…å‚æ•°

| è¶…å‚æ•° | å€¼ |
|--------|-----|
| Batch Size | 64 |
| Learning Rate | 1e-4 |
| Optimizer | AdamW (Î²1=0.9, Î²2=0.98, Îµ=1e-9) |
| Weight Decay | 0.01 |
| Gradient Clipping | 1.0 |
| Label Smoothing | 0.1 |
| Epochs | 20 |
| éšæœºç§å­ | 42 |

### æ¶ˆèå®éªŒ

æµ‹è¯•ä»¥ä¸‹å˜é‡å¯¹æ€§èƒ½çš„å½±å“ï¼š

1. **ä½ç½®ç¼–ç **: ç§»é™¤ä½ç½®ç¼–ç 
2. **æ³¨æ„åŠ›å¤´æ•°**: 2å¤´ vs 4å¤´ vs 8å¤´
3. **æ¨¡å‹å±‚æ•°**: 2å±‚ vs 4å±‚ vs 6å±‚
4. **Dropout**: 0.0 vs 0.1 vs 0.3
5. **æ¨¡å‹è§„æ¨¡**: d_model=128 vs 256
6. **æ ‡ç­¾å¹³æ»‘**: æœ‰ vs æ— 

## ğŸ“ˆ ç»“æœç¤ºä¾‹

è®­ç»ƒå®Œæˆåï¼Œ`results/` ç›®å½•å°†åŒ…å«ï¼š

- `transformer_encoder_curves.png`: è®­ç»ƒ/éªŒè¯æŸå¤±æ›²çº¿
- `transformer_encoder_log.json`: è¯¦ç»†è®­ç»ƒæ—¥å¿—
- `ablation_results.csv`: æ¶ˆèå®éªŒç»“æœè¡¨æ ¼
- `ablation_comparison.png`: æ¶ˆèå®éªŒå¯¹æ¯”å›¾

**é¢„æœŸç»“æœ** (WikiText-2, Encoder-only):
- Test Loss: ~4.5-5.0
- Test Perplexity: ~90-150

## ğŸ’» ç¡¬ä»¶è¦æ±‚

**æœ€ä½é…ç½®**:
- CPU: 4æ ¸
- å†…å­˜: 8GB
- å­˜å‚¨: 2GB

**æ¨èé…ç½®**:
- GPU: NVIDIA GPU with 4GB+ VRAM (CUDAæ”¯æŒ)
- CPU: 8æ ¸
- å†…å­˜: 16GB
- å­˜å‚¨: 5GB

**è®­ç»ƒæ—¶é—´ä¼°è®¡** (é»˜è®¤é…ç½®):
- CPU: ~4-6å°æ—¶/20 epochs
- GPU (RTX 3060): ~30-45åˆ†é’Ÿ/20 epochs

## ğŸ“ å®Œæ•´å‘½ä»¤è¡Œç¤ºä¾‹

```bash
# 1. è®¾ç½®ç¯å¢ƒ
conda create -n transformer python=3.10
conda activate transformer
pip install -r requirements.txt

# 2. ä¸‹è½½æ•°æ®
python scripts/download_data.py

# 3. è®­ç»ƒEncoderæ¨¡å‹ (å¯å¤ç°)
python src/train.py \
  --data_dir data \
  --mode encoder \
  --d_model 256 \
  --n_heads 4 \
  --n_layers 4 \
  --d_ff 1024 \
  --dropout 0.1 \
  --batch_size 64 \
  --epochs 20 \
  --lr 0.0001 \
  --weight_decay 0.01 \
  --clip_grad 1.0 \
  --label_smoothing 0.1 \
  --seed 42 \
  --exp_name transformer_encoder

# 4. è®­ç»ƒEncoder-Decoderæ¨¡å‹
python src/train.py \
  --data_dir data \
  --mode seq2seq \
  --d_model 256 \
  --n_heads 4 \
  --n_layers 4 \
  --d_ff 1024 \
  --dropout 0.1 \
  --batch_size 64 \
  --epochs 20 \
  --lr 0.0001 \
  --seed 42 \
  --exp_name transformer_seq2seq

# 5. è¿è¡Œæ¶ˆèå®éªŒ
python src/ablation.py \
  --data_dir data \
  --mode encoder \
  --epochs 15 \
  --seed 42
```

## ğŸ” ä»£ç å…³é”®å®ç°

### Multi-Head Attention

```python
class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, n_heads, dropout=0.1):
        super().__init__()
        self.d_k = d_model // n_heads
        self.n_heads = n_heads
        
        self.W_Q = nn.Linear(d_model, d_model)
        self.W_K = nn.Linear(d_model, d_model)
        self.W_V = nn.Linear(d_model, d_model)
        self.W_O = nn.Linear(d_model, d_model)
        
    def forward(self, Q, K, V, mask=None):
        # çº¿æ€§å˜æ¢å¹¶åˆ†å‰²æˆå¤šå¤´
        Q = self.W_Q(Q).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)
        K = self.W_K(K).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)
        V = self.W_V(V).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)
        
        # è®¡ç®—æ³¨æ„åŠ›
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)
        attn = F.softmax(scores, dim=-1)
        
        # åŠ æƒæ±‚å’Œå¹¶æ‹¼æ¥
        output = torch.matmul(attn, V)
        output = output.transpose(1, 2).contiguous().view(batch_size, -1, d_model)
        return self.W_O(output)
```

## ğŸ› å¸¸è§é—®é¢˜

**Q: ä¸‹è½½æ•°æ®é›†å¤±è´¥ï¼Ÿ**
A: å¯ä»¥æ‰‹åŠ¨ä» https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-raw-v1.zip ä¸‹è½½

**Q: CUDA out of memoryï¼Ÿ**
A: å‡å°batch_size (å¦‚æ”¹ä¸º32æˆ–16)ï¼Œæˆ–å‡å°æ¨¡å‹è§„æ¨¡ (d_model=128)

**Q: è®­ç»ƒé€Ÿåº¦å¤ªæ…¢ï¼Ÿ**
A: ç¡®ä¿ä½¿ç”¨GPUè®­ç»ƒã€‚æ£€æŸ¥ `torch.cuda.is_available()` æ˜¯å¦è¿”å›True

**Q: Lossä¸ä¸‹é™ï¼Ÿ**
A: æ£€æŸ¥å­¦ä¹ ç‡æ˜¯å¦åˆé€‚ï¼Œå¯ä»¥å°è¯•é™ä½åˆ°5e-5ï¼›ç¡®ä¿æ•°æ®é¢„å¤„ç†æ­£ç¡®

## ğŸ“š å‚è€ƒæ–‡çŒ®

[1] Vaswani, A., et al. (2017). Attention is all you need. *Advances in Neural Information Processing Systems*, 30.

[2] Merity, S., et al. (2016). Pointer sentinel mixture models. *arXiv preprint arXiv:1609.07843*.

## ğŸ‘¤ ä½œè€…ä¿¡æ¯

- **å§“å**: [Your Name]
- **å­¦å·**: [Your Student ID]
- **è¯¾ç¨‹**: å¤§æ¨¡å‹åŸºç¡€ä¸åº”ç”¨
- **å­¦æœŸ**: 2025å¹´ç§‹å­£

## ğŸ“„ è®¸å¯è¯

MIT License

## ğŸ™ è‡´è°¢

æ„Ÿè°¢Salesforce Researchæä¾›WikiText-2æ•°æ®é›†ï¼Œæ„Ÿè°¢PyTorchå›¢é˜Ÿæä¾›ä¼˜ç§€çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ã€‚

---

**æœ€åæ›´æ–°**: 2025å¹´10æœˆ
